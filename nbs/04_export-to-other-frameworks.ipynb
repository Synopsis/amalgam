{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastai2.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rahulsomani/anaconda3/lib/python3.7/site-packages/torch/serialization.py:657: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/Users/rahulsomani/anaconda3/lib/python3.7/site-packages/torch/serialization.py:657: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/Users/rahulsomani/anaconda3/lib/python3.7/site-packages/torch/serialization.py:657: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "class ApplyPILFilter(RandTransform): pass\n",
    "path_model = '/Users/rahulsomani/Desktop/shot-lighting-cast/fastai2-110-epoch-model.pkl'\n",
    "\n",
    "learn = load_learner(path_model);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to change fastai's `Flatten` to `nn.Flatten()` for CoreML compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(1,3,224,224)\n",
    "out1 = learn.model(x)\n",
    "learn.model[1][1] = nn.Flatten()\n",
    "out2 = learn.model(x)\n",
    "\n",
    "torch.equal(out1,out2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing & Predicting in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as TTF\n",
    "import torch.nn as nn\n",
    "to_cuda = lambda x: x.cuda() if torch.cuda.is_available() else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagenet_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PathLike = Union[str,Path]\n",
    "\n",
    "def preprocess_one(fname:PathLike):\n",
    "    x = open_image(fname)\n",
    "    x = TTF.to_tensor(x)\n",
    "    x = TTF.normalize(x, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    x = to_cuda(x)\n",
    "    x = x.unsqueeze(0)\n",
    "    return x\n",
    "\n",
    "def preprocess_batch(fnames:Union[PathLike,Collection]):\n",
    "    batch = [preprocess_one(f) for f in fnames]\n",
    "    return torch.cat(batch)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_imgs = Path('/Users/rahulsomani/Desktop/lighting-cast/')\n",
    "files = get_image_files(path_imgs)\n",
    "f = files[120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import PIL\n",
    "\n",
    "open_image = lambda f,size=(224,224): PIL.Image.open(f).convert('RGB').resize(size, PIL.Image.BILINEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7403, 0.2597]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = preprocess_one(f)\n",
    "torch_pred = nn.Softmax(-1)(learn.model(x))\n",
    "torch_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnx.utils\n",
    "from onnx import optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_to_onnx(model:nn.Module,\n",
    "                  activation:nn.Module=None,\n",
    "                  save_path:str     = '../exported-models/',\n",
    "                  model_fname:str   = 'onnx-model',\n",
    "                  input_shape:tuple = (1,3,224,224),\n",
    "                  input_name:str    = 'input_image',\n",
    "                  output_names:Union[str,list] = 'output',\n",
    "                  **export_args) -> None:\n",
    "    save_path = Path(save_path)\n",
    "    if isinstance(output_names, str): output_names = [output_names]\n",
    "    if activation: model = nn.Sequential(*[model, activation])\n",
    "    model.eval()\n",
    "    x = torch.randn(input_shape, requires_grad=True)\n",
    "    x = x.cuda() if torch.cuda.is_available() else x\n",
    "    model(x)\n",
    "    dynamic_batch = {0: 'batch'}\n",
    "    dynamic_axes  = {input_name : dynamic_batch}\n",
    "    for out in output_names: dynamic_axes[out] = dynamic_batch\n",
    "    torch.onnx._export(model, x, f\"{save_path/model_fname}.onnx\",\n",
    "                       export_params=True, verbose=False,\n",
    "                       input_names=[input_name], output_names=output_names,\n",
    "                       dynamic_axes=dynamic_axes, keep_initializers_as_inputs=True,\n",
    "                       **export_args)\n",
    "    print(f\"Loading, polishing, and optimising exported model from {save_path/model_fname}.onnx\")\n",
    "    onnx_model = onnx.load(f'{save_path/model_fname}.onnx')\n",
    "    model = onnx.utils.polish_model(onnx_model)\n",
    "    #onnx.checker.check_model(model)\n",
    "\n",
    "    # removing unused parts of the model\n",
    "    passes = [\"extract_constant_to_initializer\", \"eliminate_unused_initializer\"]\n",
    "    optimized_model = optimizer.optimize(onnx_model, passes)\n",
    "\n",
    "    onnx.save(optimized_model, f'{save_path/model_fname}.onnx')\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading, polishing, and optimising exported model from /Users/rahulsomani/Desktop/lighting-cast.onnx\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "torch_to_onnx(model = learn.model,\n",
    "              activation   = nn.Softmax(-1),\n",
    "              save_path    = '/Users/rahulsomani/Desktop/',\n",
    "              model_fname  = 'lighting-cast',\n",
    "              output_names = 'lighting-cast')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime import InferenceSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_to_numpy(tensor): return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = '/Users/rahulsomani/Desktop/lighting-cast.onnx'\n",
    "session = InferenceSession(onnx_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.74026996, 0.25973007]], dtype=float32)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = {session.get_inputs()[0].name:\n",
    "     torch_to_numpy(preprocess_one(f))}\n",
    "\n",
    "preds_onnx = session.run(None, x)\n",
    "preds_onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch vs Single Image Prediction Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403 ms ± 74.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "x = {session.get_inputs()[0].name:\n",
    "     torch_to_numpy(preprocess_batch(files[:10]))}\n",
    "\n",
    "preds_onnx = session.run(None, x)\n",
    "#preds_onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340 ms ± 16.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "for f in files[:10]:\n",
    "    x = {session.get_inputs()[0].name:\n",
    "         torch_to_numpy(preprocess_one(f))}\n",
    "    preds_onnx = session.run(None, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/onnx/onnx-tensorflow/blob/master/example/onnx_to_tf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnx_tf.backend import prepare\n",
    "\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "output = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to CoreML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagenet_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01735207357279195"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.0 / (255.0 * 0.226)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:TensorFlow version 2.2.0 detected. Last version known to be fully compatible is 1.14.0 .\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import coremltools\n",
    "import onnx_coreml\n",
    "import os\n",
    "\n",
    "from onnx_coreml import convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coremltools version: 3.3\n",
      "onnx-coreml version: 1.3\n"
     ]
    }
   ],
   "source": [
    "print(f\"coremltools version: {coremltools.__version__}\")\n",
    "print(f\"onnx-coreml version: {onnx_coreml.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagenet_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/157: Converting Node Type Conv\n",
      "2/157: Converting Node Type BatchNormalization\n",
      "3/157: Converting Node Type Clip\n",
      "4/157: Converting Node Type Conv\n",
      "5/157: Converting Node Type BatchNormalization\n",
      "6/157: Converting Node Type Clip\n",
      "7/157: Converting Node Type Conv\n",
      "8/157: Converting Node Type BatchNormalization\n",
      "9/157: Converting Node Type Conv\n",
      "10/157: Converting Node Type BatchNormalization\n",
      "11/157: Converting Node Type Clip\n",
      "12/157: Converting Node Type Conv\n",
      "13/157: Converting Node Type BatchNormalization\n",
      "14/157: Converting Node Type Clip\n",
      "15/157: Converting Node Type Conv\n",
      "16/157: Converting Node Type BatchNormalization\n",
      "17/157: Converting Node Type Conv\n",
      "18/157: Converting Node Type BatchNormalization\n",
      "19/157: Converting Node Type Clip\n",
      "20/157: Converting Node Type Conv\n",
      "21/157: Converting Node Type BatchNormalization\n",
      "22/157: Converting Node Type Clip\n",
      "23/157: Converting Node Type Conv\n",
      "24/157: Converting Node Type BatchNormalization\n",
      "25/157: Converting Node Type Add\n",
      "26/157: Converting Node Type Conv\n",
      "27/157: Converting Node Type BatchNormalization\n",
      "28/157: Converting Node Type Clip\n",
      "29/157: Converting Node Type Conv\n",
      "30/157: Converting Node Type BatchNormalization\n",
      "31/157: Converting Node Type Clip\n",
      "32/157: Converting Node Type Conv\n",
      "33/157: Converting Node Type BatchNormalization\n",
      "34/157: Converting Node Type Conv\n",
      "35/157: Converting Node Type BatchNormalization\n",
      "36/157: Converting Node Type Clip\n",
      "37/157: Converting Node Type Conv\n",
      "38/157: Converting Node Type BatchNormalization\n",
      "39/157: Converting Node Type Clip\n",
      "40/157: Converting Node Type Conv\n",
      "41/157: Converting Node Type BatchNormalization\n",
      "42/157: Converting Node Type Add\n",
      "43/157: Converting Node Type Conv\n",
      "44/157: Converting Node Type BatchNormalization\n",
      "45/157: Converting Node Type Clip\n",
      "46/157: Converting Node Type Conv\n",
      "47/157: Converting Node Type BatchNormalization\n",
      "48/157: Converting Node Type Clip\n",
      "49/157: Converting Node Type Conv\n",
      "50/157: Converting Node Type BatchNormalization\n",
      "51/157: Converting Node Type Add\n",
      "52/157: Converting Node Type Conv\n",
      "53/157: Converting Node Type BatchNormalization\n",
      "54/157: Converting Node Type Clip\n",
      "55/157: Converting Node Type Conv\n",
      "56/157: Converting Node Type BatchNormalization\n",
      "57/157: Converting Node Type Clip\n",
      "58/157: Converting Node Type Conv\n",
      "59/157: Converting Node Type BatchNormalization\n",
      "60/157: Converting Node Type Conv\n",
      "61/157: Converting Node Type BatchNormalization\n",
      "62/157: Converting Node Type Clip\n",
      "63/157: Converting Node Type Conv\n",
      "64/157: Converting Node Type BatchNormalization\n",
      "65/157: Converting Node Type Clip\n",
      "66/157: Converting Node Type Conv\n",
      "67/157: Converting Node Type BatchNormalization\n",
      "68/157: Converting Node Type Add\n",
      "69/157: Converting Node Type Conv\n",
      "70/157: Converting Node Type BatchNormalization\n",
      "71/157: Converting Node Type Clip\n",
      "72/157: Converting Node Type Conv\n",
      "73/157: Converting Node Type BatchNormalization\n",
      "74/157: Converting Node Type Clip\n",
      "75/157: Converting Node Type Conv\n",
      "76/157: Converting Node Type BatchNormalization\n",
      "77/157: Converting Node Type Add\n",
      "78/157: Converting Node Type Conv\n",
      "79/157: Converting Node Type BatchNormalization\n",
      "80/157: Converting Node Type Clip\n",
      "81/157: Converting Node Type Conv\n",
      "82/157: Converting Node Type BatchNormalization\n",
      "83/157: Converting Node Type Clip\n",
      "84/157: Converting Node Type Conv\n",
      "85/157: Converting Node Type BatchNormalization\n",
      "86/157: Converting Node Type Add\n",
      "87/157: Converting Node Type Conv\n",
      "88/157: Converting Node Type BatchNormalization\n",
      "89/157: Converting Node Type Clip\n",
      "90/157: Converting Node Type Conv\n",
      "91/157: Converting Node Type BatchNormalization\n",
      "92/157: Converting Node Type Clip\n",
      "93/157: Converting Node Type Conv\n",
      "94/157: Converting Node Type BatchNormalization\n",
      "95/157: Converting Node Type Conv\n",
      "96/157: Converting Node Type BatchNormalization\n",
      "97/157: Converting Node Type Clip\n",
      "98/157: Converting Node Type Conv\n",
      "99/157: Converting Node Type BatchNormalization\n",
      "100/157: Converting Node Type Clip\n",
      "101/157: Converting Node Type Conv\n",
      "102/157: Converting Node Type BatchNormalization\n",
      "103/157: Converting Node Type Add\n",
      "104/157: Converting Node Type Conv\n",
      "105/157: Converting Node Type BatchNormalization\n",
      "106/157: Converting Node Type Clip\n",
      "107/157: Converting Node Type Conv\n",
      "108/157: Converting Node Type BatchNormalization\n",
      "109/157: Converting Node Type Clip\n",
      "110/157: Converting Node Type Conv\n",
      "111/157: Converting Node Type BatchNormalization\n",
      "112/157: Converting Node Type Add\n",
      "113/157: Converting Node Type Conv\n",
      "114/157: Converting Node Type BatchNormalization\n",
      "115/157: Converting Node Type Clip\n",
      "116/157: Converting Node Type Conv\n",
      "117/157: Converting Node Type BatchNormalization\n",
      "118/157: Converting Node Type Clip\n",
      "119/157: Converting Node Type Conv\n",
      "120/157: Converting Node Type BatchNormalization\n",
      "121/157: Converting Node Type Conv\n",
      "122/157: Converting Node Type BatchNormalization\n",
      "123/157: Converting Node Type Clip\n",
      "124/157: Converting Node Type Conv\n",
      "125/157: Converting Node Type BatchNormalization\n",
      "126/157: Converting Node Type Clip\n",
      "127/157: Converting Node Type Conv\n",
      "128/157: Converting Node Type BatchNormalization\n",
      "129/157: Converting Node Type Add\n",
      "130/157: Converting Node Type Conv\n",
      "131/157: Converting Node Type BatchNormalization\n",
      "132/157: Converting Node Type Clip\n",
      "133/157: Converting Node Type Conv\n",
      "134/157: Converting Node Type BatchNormalization\n",
      "135/157: Converting Node Type Clip\n",
      "136/157: Converting Node Type Conv\n",
      "137/157: Converting Node Type BatchNormalization\n",
      "138/157: Converting Node Type Add\n",
      "139/157: Converting Node Type Conv\n",
      "140/157: Converting Node Type BatchNormalization\n",
      "141/157: Converting Node Type Clip\n",
      "142/157: Converting Node Type Conv\n",
      "143/157: Converting Node Type BatchNormalization\n",
      "144/157: Converting Node Type Clip\n",
      "145/157: Converting Node Type Conv\n",
      "146/157: Converting Node Type BatchNormalization\n",
      "147/157: Converting Node Type Conv\n",
      "148/157: Converting Node Type BatchNormalization\n",
      "149/157: Converting Node Type Clip\n",
      "150/157: Converting Node Type GlobalAveragePool\n",
      "151/157: Converting Node Type Flatten\n",
      "152/157: Converting Node Type BatchNormalization\n",
      "153/157: Converting Node Type MatMul\n",
      "154/157: Converting Node Type Relu\n",
      "155/157: Converting Node Type BatchNormalization\n",
      "156/157: Converting Node Type MatMul\n",
      "157/157: Converting Node Type Softmax\n",
      "Translation to CoreML spec completed. Now compiling the CoreML model.\n",
      "Model Compilation done.\n"
     ]
    }
   ],
   "source": [
    "preprocessing_args = dict(\n",
    "    image_scale = 1/255.,\n",
    "    red_bias    = -0.485,\n",
    "    green_bias  = -0.456,\n",
    "    blue_bias   = -0.406,\n",
    "    is_bgr      = False\n",
    ")\n",
    "\n",
    "red_sdev, green_sdev, blue_sdev = imagenet_stats[1]\n",
    "\n",
    "coreml_model = convert(\n",
    "    model = '/Users/rahulsomani/Desktop/lighting-cast.onnx',\n",
    "    mode  = 'classifier',\n",
    "    class_labels = list(learn.dls.vocab),\n",
    "    image_input_names  = ['input_image'],\n",
    "    preprocessing_args = preprocessing_args,\n",
    "    minimum_ios_deployment_target = '11.2'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# borrowed from the CoreML Survival Guide, written by Matthijs Hollemans\n",
    "def get_nn_spec(spec):\n",
    "    \"spec is of type `Model_pb2.Model`, accessed via coreml_model.get_spec()\"\n",
    "    if   spec.WhichOneof(\"Type\") == 'neuralNetwork': return spec.neuralNetwork\n",
    "    elif spec.WhichOneof(\"Type\") == 'neuralNetworkClassifier': return spec.neuralNetworkClassifier\n",
    "    elif spec.WhichOneof(\"Type\") == 'neuralNetworkRegressor':  return spec.neuralNetworkRegressor\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = coreml_model.get_spec()\n",
    "nn   = get_nn_spec(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store away old layers to add back to the reconstructed network\n",
    "old_layers = copy.deepcopy(nn.layers)\n",
    "del nn.layers[:]\n",
    "\n",
    "# names of inputs and outputs of the scaling layer\n",
    "input_name  = old_layers[0].input[0]\n",
    "output_name = f\"{input_name}_scaled\"\n",
    "\n",
    "# create and add scaling layer to new network\n",
    "scale_layer = nn.layers.add()\n",
    "scale_layer.name = \"scale_layer\"\n",
    "scale_layer.input.append(input_name)\n",
    "scale_layer.output.append(output_name)\n",
    "\n",
    "scale_layer.scale.scale.floatValue.extend([\n",
    "    1/red_sdev, 1/green_sdev, 1/blue_sdev\n",
    "])\n",
    "scale_layer.scale.shapeScale.extend([3,1,1])\n",
    "\n",
    "# add back all the old layers\n",
    "nn.layers.extend(old_layers)\n",
    "nn.layers[1].input[0] = output_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreml_path = '/Users/rahulsomani/Desktop/lighting-cast.mlmodel'\n",
    "coremltools.utils.save_spec(spec, coreml_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoreML Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreml_model = coremltools.models.MLModel(coreml_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classLabel': 'shot_lighting_cast_hard',\n",
      " 'lighting-cast': {'shot_lighting_cast_hard': 0.7405732274055481,\n",
      "                   'shot_lighting_cast_soft': 0.2594267427921295}}\n"
     ]
    }
   ],
   "source": [
    "coreml_preds = coreml_model.predict({'input_image': open_image(f)})\n",
    "pprint(coreml_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Preds Across Frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fastai_pred = learn.predict(f)[-1]\n",
    "coreml_preds = [v for k,v in coreml_preds['lighting-cast'].items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastAI:  (0.7402700185775757, 0.2597300112247467)\n",
      "PyTorch: (0.7402703166007996, 0.25972968339920044)\n",
      "ONNX:    (0.74026996, 0.25973007)\n",
      "CoreML:  (0.7405732274055481, 0.2594267427921295)\n"
     ]
    }
   ],
   "source": [
    "print(f\"FastAI:  {fastai_pred[0].item(), fastai_pred[1].item()}\")\n",
    "print(f\"PyTorch: {torch_pred[0][0].item(), torch_pred[0][1].item()}\")\n",
    "print(f\"ONNX:    {preds_onnx[0][0][0], preds_onnx[0][0][1]}\")\n",
    "print(f\"CoreML:  {coreml_preds[0], coreml_preds[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
