{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp export.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.18'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "import fastai\n",
    "fastai.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "from fastai.vision.all import *\n",
    "\n",
    "import os\n",
    "import onnx\n",
    "import onnx.utils\n",
    "from onnx import optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standalone Torch -> ONNX Exporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def torch_to_onnx(model:nn.Module,\n",
    "                  activation:nn.Module=None,\n",
    "                  save_path:str     = '../exported-models/',\n",
    "                  model_fname:str   = 'onnx-model',\n",
    "                  input_shape:tuple = (1,3,224,224),\n",
    "                  input_name:str    = 'input_image',\n",
    "                  output_names:Union[str,list] = 'output',\n",
    "                  verbose:bool = True,\n",
    "                  **export_args) -> os.PathLike:\n",
    "    \"\"\"\n",
    "    Export a `nn.Module` -> ONNX\n",
    "    This function exports the model with support for batching,\n",
    "    checks that the export was done properly, and polishes the\n",
    "    model up (removes unnecessary fluff added during conversion)\n",
    "\n",
    "    The path to the saved model is returned\n",
    "    Key Arguments\n",
    "    =============\n",
    "    * activation:  If not None, append this to the end of your model.\n",
    "                   Typically a `nn.Softmax(-1)` or `nn.Sigmoid()`\n",
    "    * input_shape: Shape of the inputs to the model\n",
    "    \"\"\"\n",
    "    save_path = Path(save_path)\n",
    "    if isinstance(output_names, str): output_names = [output_names]\n",
    "    if activation: model = nn.Sequential(*[model, activation])\n",
    "    model.eval()\n",
    "    x = torch.randn(input_shape, requires_grad=True)\n",
    "    x = x.cuda() if torch.cuda.is_available() else x\n",
    "    model(x)\n",
    "    dynamic_batch = {0: 'batch'}\n",
    "    dynamic_axes  = {input_name : dynamic_batch}\n",
    "    for out in output_names: dynamic_axes[out] = dynamic_batch\n",
    "    torch.onnx._export(model, x, f\"{save_path/model_fname}.onnx\",\n",
    "                       export_params=True, verbose=False,\n",
    "                       input_names=[input_name], output_names=output_names,\n",
    "                       dynamic_axes=dynamic_axes, keep_initializers_as_inputs=True,\n",
    "                       **export_args)\n",
    "    if verbose:\n",
    "        print(f\"Loading, polishing, and optimising exported model from {save_path/model_fname}.onnx\")\n",
    "    onnx_model = onnx.load(f'{save_path/model_fname}.onnx')\n",
    "    model = onnx.utils.polish_model(onnx_model)\n",
    "    #onnx.checker.check_model(model)\n",
    "\n",
    "    # removing unused parts of the model\n",
    "    passes = [\"extract_constant_to_initializer\", \"eliminate_unused_initializer\"]\n",
    "    optimized_model = optimizer.optimize(onnx_model, passes)\n",
    "\n",
    "    onnx.save(optimized_model, f'{save_path/model_fname}.onnx')\n",
    "    print('<Exported ONNX model successfully>') # print regardless\n",
    "    return f'{save_path/model_fname}.onnx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fastai Learner -> ONNX Exporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "@delegates(to=torch_to_onnx, but=[\"model\", \"save_path\"])\n",
    "def export_to_onnx(self:Learner, save_path=None, activation:Union[str,nn.Module,None]='auto', **kwargs):\n",
    "    \"\"\"Export to ONNX along with an accompanying `vocab.txt` file\n",
    "    * If `save_path` is None, model is exported to `Learner.path`\n",
    "    * If `activation`=='auto', the act function (nn.Sigmoid or nn.Softmax)\n",
    "      is determined based on `Learner.loss_func`\n",
    "    \"\"\"\n",
    "    if save_path is None: save_path = self.path\n",
    "    else: save_path = Path(save_path)\n",
    "    if activation=='auto':\n",
    "        if self.loss_func.__class__==fastai.losses.CrossEntropyLossFlat:\n",
    "            activation = nn.Softmax(-1)\n",
    "        elif self.loss_func.__class__==fastai.losses.BCEWithLogitsLossFlat:\n",
    "            activation = nn.Sigmoid()\n",
    "    \n",
    "    with open(save_path/\"vocab.txt\", \"w\") as f:\n",
    "        f.write(', '.join(self.dls.vocab))\n",
    "        print(f\"Wrote 'vocab.txt' file to {save_path}\")\n",
    "    torch_to_onnx(self.model, save_path=save_path, activation=activation, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 'vocab.txt' file to /tmp\n",
      "Loading, polishing, and optimising exported model from /tmp/onnx-model.onnx\n",
      "<Exported ONNX model successfully>\n"
     ]
    }
   ],
   "source": [
    "learn.export_to_onnx(\"/tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted export_onnx.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script('export_onnx.ipynb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FastAI_2.1",
   "language": "python",
   "name": "fastai_2.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
