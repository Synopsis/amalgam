{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp loss_funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FocalLoss(nn.Module):\n",
    "    # implementation adapted from https://amaarora.github.io/2020/06/29/FocalLoss.html\n",
    "    # paper: https://arxiv.org/abs/1708.02002\n",
    "    \"Focal Loss\"\n",
    "    def __init__(self, alpha=.25, gamma=2, reduction='none', pos_weight=None):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = torch.tensor([alpha, 1-alpha])#.cuda()\n",
    "        self.gamma = gamma\n",
    "        self.reduction  = reduction\n",
    "        self.pos_weight = pos_weight\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets,\n",
    "                                                      reduction=self.reduction,\n",
    "                                                      pos_weight=self.pos_weight)\n",
    "        targets = targets.type(torch.long)\n",
    "        at = self.alpha.gather(0, targets.data.view(-1))\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        #F_loss = at*(1-pt)**self.gamma * BCE_loss\n",
    "        F_loss = at*(1-pt.view(-1))**self.gamma * BCE_loss.view(-1)\n",
    "        return F_loss.mean()\n",
    "@log_args\n",
    "@delegates()\n",
    "class FocalLossFlat(BaseLoss):\n",
    "    @use_kwargs_dict(keep=True, alpha=.25, gamma=2., reduction='none', pos_weight=None)\n",
    "    def __init__(self, thresh=0.5, axis=-1, floatify=True, **kwargs):\n",
    "        super().__init__(FocalLoss, axis=axis, floatify=floatify, is_2d=False, flatten=False, **kwargs)\n",
    "        self.thresh = thresh\n",
    "\n",
    "    def decodes(self, x):    return x > self.thresh\n",
    "    def activation(self, x): return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "vocab_size = 10\n",
    "pos_weight = torch.ones(vocab_size)\n",
    "\n",
    "yb, preds = torch.rand(64,vocab_size), torch.rand(64,vocab_size)\n",
    "\n",
    "focal_loss     = FocalLoss(pos_weight=pos_weight)\n",
    "focal_loss_BCE = FocalLoss(alpha=1, gamma=0, pos_weight=pos_weight)\n",
    "BCEloss    = nn.BCEWithLogitsLoss(reduction='mean', pos_weight=pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0505)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focal_loss(yb,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert focal_loss_BCE(yb,preds) == BCEloss(yb,preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FastAI_2.1",
   "language": "python",
   "name": "fastai_2.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
